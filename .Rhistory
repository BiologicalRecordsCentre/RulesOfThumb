name='Trend output') +
facet_wrap(~Taxa_Root)  +
geom_segment(aes(x=logit(0.2), y = log10(8.7), xend = logit(0.99), yend = log10(8.7)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(8.7), xend = logit(0.99), yend = log10(3.4)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(3.4), xend = logit(0.99999), yend = log10(3.4)), color = 'black')
ggplot(data = trendsData, aes(x = logit(prop_abs), y = log10(P90),
col = as.factor(bad))) +
geom_point() +
scale_color_manual(values=c('#ca002035', "#0571b035"),
name='Trend output') +
facet_wrap(~Taxa_Root)  +
geom_segment(aes(x=logit(0.2), y = log10(8.7), xend = logit(0.99), yend = log10(8.7)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(8.7), xend = logit(0.99), yend = log10(3.4)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(3.4), xend = logit(0.99999), yend = log10(3.4)), color = 'black')
ggplot(data = trendsData, aes(x = logit(prop_abs), y = log10(P90),
col = as.factor(bad))) +
geom_point() +
scale_color_manual(values=c('#ca002035', "#0571b035"),
name='Trend output') +
facet_wrap(~Taxa_Root)  +
geom_segment(aes(x=logit(0.2), y = log10(8.7), xend = logit(0.99), yend = log10(8.7)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(8.7), xend = logit(0.99), yend = log10(3.4)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(3.4), xend = logit(0.99999), yend = log10(3.4)), color = 'black') +
theme_bw()
ggplot(data = trendsData, aes(x = logit(prop_abs), y = log10(P90),
col = as.factor(bad))) +
geom_point() +
scale_color_manual(values=c("#0571b035", '#ca002035'),
name='Trend output') +
facet_wrap(~Taxa_Root)  +
geom_segment(aes(x=logit(0.2), y = log10(8.7), xend = logit(0.99), yend = log10(8.7)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(8.7), xend = logit(0.99), yend = log10(3.4)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(3.4), xend = logit(0.99999), yend = log10(3.4)), color = 'black') +
theme_bw()
ggplot(data = trendsData, aes(x = logit(prop_abs), y = log10(P90),
col = as.factor(bad))) +
geom_point() +
scale_color_manual(values=c("#0571b035", '#ca002035'),
name='Trend output') +
facet_wrap(~Taxa)  +
geom_segment(aes(x=logit(0.2), y = log10(8.7), xend = logit(0.99), yend = log10(8.7)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(8.7), xend = logit(0.99), yend = log10(3.4)), color = 'black') +
geom_segment(aes(x=logit(0.99), y = log10(3.4), xend = logit(0.99999), yend = log10(3.4)), color = 'black') +
theme_bw()
levels(trendsData$Taxa)
#plot(pfit_bad, uniform=TRUE,
#  	main="'Rules of thumb' for species occupancy modelling, 1:10 good:bad",
#  	margin = .1)
#text(pfit_bad, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit_bad, extra = 108, under = F, type = 3, clip.right.labs = FALSE, branch=0.7)
## Data wrangling complete, plot it up
ggplot(td_taxa, aes(Taxa, P90), main = 'Title') +
geom_jitter(aes(reorder(cat,desc(cat)), P90), data = td_taxa,
colour = td_taxa$colour, alpha = td_taxa$alpha,
position = position_jitter(width = 0.08)) +
scale_y_log10() +
geom_hline(yintercept=8.65) +
theme(axis.text.x = element_text(angle=90, hjust=1, vjust = 1.25),
axis.ticks.x = element_blank(), axis.ticks.y = element_blank()) +
scale_x_discrete(labels=rev(names_long)) +
coord_flip()
substring(trendsData$Taxa_Root)
substring(trendsData$Taxa_Root,1,4)
substring(trendsData$Taxa_Root,1,4) == "last"
sum(substring(trendsData$Taxa_Root,1,4) == "last")
sum(substring(trendsData$Taxa_Root,1,4) == "Last")
sum(substring(trendsData$Taxa,1,4) == "Last")
sum(substring(trendsData$Taxa,1,4) == "last")
sum(substring(trendsData$Taxa,1,8) == "last10yr")
trendsData$last10yr <- substring(trendsData$Taxa,1,8) == "last10yr"
trendsData2 <- trendsData[trendsData$last10yr == T]
trendsData2 <- trendsData[trendsData$last10yr == T,]
trendsData2$bad <- rep('bad',nrow(trendsData2))
trendsData2$bad[(trendsData2$PropYrConverged < 0.3437 &
trendsData2$mean_year_precision >= 149.3) |
(trendsData2$PropYrConverged >= 0.3437 &
trendsData2$mean_year_precision >= 60.02)] <- ' good'
# Perform the fit.  The loss parameter in the rpart function determines the
# weighting:
#   By increasing 'goodweight', this increases the importance of correctly
#   identifying good models and not assigning bad models to the good category.
#   However, this is at the expense of assigning many good models to the bad
#   category.
#   By increasing 'badweight', the inverse is true
badweight <- 1
goodweight <- 1
fit <- rpart(bad ~ median + P90 + zmedian + zP90 +
visits_median + visits_P70 + visits_P80 + visits_P90 +
prop_of_years + prop_repeats_grp + prop_list_one +
prop_abs,
method = "class",
data = trendsData,
parms = list(loss = matrix(c(0,goodweight,badweight,0),ncol = 2)))
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
bad
fit <- rpart(bad ~ median + P90 + zmedian + zP90 +
visits_median + visits_P70 + visits_P80 + visits_P90 +
prop_of_years + prop_repeats_grp + prop_list_one +
prop_abs,
method = "class",
data = trendsData2,
parms = list(loss = matrix(c(0,goodweight,badweight,0),ncol = 2)))
printcp(fit) # display the results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main = "'Rules of thumb' for species occupancy modelling",
margin = .1)
text(fit, use.n = TRUE,
all = TRUE,
cex = .7)
# prune the tree
pfit <- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 3, clip.right.labs = FALSE, branch=0.7)
trendsData2 <- trendsData[trendsData$last10yr == F,]
trendsData2$bad <- rep('bad',nrow(trendsData2))
trendsData2$bad[(trendsData2$PropYrConverged < 0.3437 &
trendsData2$mean_year_precision >= 149.3) |
(trendsData2$PropYrConverged >= 0.3437 &
trendsData2$mean_year_precision >= 60.02)] <- ' good'
# Perform the fit.  The loss parameter in the rpart function determines the
# weighting:
#   By increasing 'goodweight', this increases the importance of correctly
#   identifying good models and not assigning bad models to the good category.
#   However, this is at the expense of assigning many good models to the bad
#   category.
#   By increasing 'badweight', the inverse is true
badweight <- 1
goodweight <- 1
fit <- rpart(bad ~ median + P90 + zmedian + zP90 +
visits_median + visits_P70 + visits_P80 + visits_P90 +
prop_of_years + prop_repeats_grp + prop_list_one +
prop_abs,
method = "class",
data = trendsData2,
parms = list(loss = matrix(c(0,goodweight,badweight,0),ncol = 2)))
printcp(fit) # display the results
summary(fit) # detailed summary of splits
# prune the tree
pfit <- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 3, clip.right.labs = FALSE, branch=0.7)
trendsData2 <- trendsData[trendsData$P90>0,]
trendsData2$bad <- rep('bad',nrow(trendsData2))
trendsData2$bad[(trendsData2$PropYrConverged < 0.3437 &
trendsData2$mean_year_precision >= 149.3) |
(trendsData2$PropYrConverged >= 0.3437 &
trendsData2$mean_year_precision >= 60.02)] <- ' good'
# Perform the fit.  The loss parameter in the rpart function determines the
# weighting:
#   By increasing 'goodweight', this increases the importance of correctly
#   identifying good models and not assigning bad models to the good category.
#   However, this is at the expense of assigning many good models to the bad
#   category.
#   By increasing 'badweight', the inverse is true
badweight <- 1
fit <- rpart(bad ~ median + P90 + zmedian + zP90 +
visits_median + visits_P70 + visits_P80 + visits_P90 +
prop_of_years + prop_repeats_grp + prop_list_one +
prop_abs,
method = "class",
data = trendsData2,
parms = list(loss = matrix(c(0,goodweight,badweight,0),ncol = 2)))
printcp(fit) # display the results
# prune the tree
pfit <- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 3, clip.right.labs = FALSE, branch=0.7)
sum(trendsData$P90>0)
sum(trendsData$P90==0)
trendsData$last10yr <- substring(trendsData$Taxa,1,8) == "last10yr")
sum(trendsData$P90==0)
trendsData[trendsData$P90>0,]
trendsData <- trendsData[trendsData$P90>0,]
# Lets have a look at the distribution
quantile(trendsData$precision_growth_rate)
ggplot(trendsData, aes(x = precision_growth_rate)) + geom_histogram(bins=20) +
scale_x_log10(labels=scales::comma) + theme_light()
quantile(trendsData$mean_year_precision)
ggplot(trendsData, aes(x = mean_year_precision)) + geom_histogram(bins=20) +
scale_x_log10(labels=scales::comma) + theme_light()
# Lets look at the results of the consultation with model experts
fsdf <- read.csv(file = 'Results/Consultation/fsdf_full.csv')
ggplot() +
geom_point(data=trendsData[trendsData$mean_year_precision>6&
trendsData$mean_year_precision<1000000&
trendsData$precision_growth_rate>0.001&
trendsData$precision_growth_rate<10,],
aes(x=precision_growth_rate,
y=mean_year_precision),
color='#DFDFDF',
size=.5) +
geom_point(data=fsdf,
aes(x=precision_growth_rate,
y=mean_year_precision,
color=as.factor(score))) +
scale_x_log10(limits=c(0.001,10),labels=scales::comma) +
scale_y_log10(limits=c(6,1000000),labels = scales::comma) +
theme_light() +
scale_color_manual(values=c('#000033','#0000ff',"#9999ff", "#ccccff"),
name='Model score') +
labs(title='What determines a "bad" model?') +
xlab("Growth Rate Precision") + ylab("Mean Year Precision")
ggplot() +
geom_point(data=trendsData[trendsData$mean_year_precision>6&
trendsData$mean_year_precision<1000000&
trendsData$precision_growth_rate>0.001&
trendsData$precision_growth_rate<10,],
aes(x=precision_growth_rate,
y=mean_year_precision),
color='#DFDFDF',
size=.5) +
geom_point(data=fsdf,
aes(x=precision_growth_rate,
y=mean_year_precision,
color=as.factor(score))) +
scale_x_log10(limits=c(0.001,10),labels=scales::comma) +
scale_y_log10(limits=c(6,1000000),labels = scales::comma) +
theme_light() +
scale_color_manual(values=c('#000033','#0000ff',"#9999ff", "#ccccff"),
name='Model score') +
labs(title='What determines a "bad" model?') +
xlab("Growth Rate Precision") + ylab("Mean Year Precision")
ggplot() +
geom_point(data=trendsData[trendsData$mean_year_precision>6&
trendsData$mean_year_precision<1000000,],
aes(x=PropYrConverged,
y=mean_year_precision),
color='#DFDFDF',
size=.5) +
geom_point(data=fsdf,
aes(x=PropYrConverged,
y=mean_year_precision,
color=as.factor(score))) +
scale_x_continuous(limits=c(0,1),labels=scales::comma) +
scale_y_log10(limits=c(6,1000000),labels = scales::comma) +
theme_light() +
scale_color_manual(values=c('#000033','#0000ff',"#9999ff", "#ccccff"),
name='Model score') +
labs(title='What determines a "bad" model?') +
xlab("Proportion of Years which Converged") + ylab("Mean Year Precision")
fsdf$good <- rep('bad',nrow(fsdf))
fsdf$good[fsdf$score>=2] <- 'good'
fit_fsdf <- rpart(good ~ mean_year_precision + precision_growth_rate,
method = 'class',
data = fsdf)
rpart.plot(fit_fsdf)
rpart.plot(fit_fsdf, extra = 108)
rpart.plot(fit_fsdf, extra = 108, type = 4, clip.right.labs = FALSE)
rpart.plot(fit_fsdf, extra = 108, type = 5, clip.right.labs = FALSE)
rpart.plot(fit_fsdf, extra = 108, type = 3, clip.right.labs = FALSE)
rpart.plot(fit_fsdf, extra = 101, type = 4)
rpart.plot(fit_fsdf, extra = 101, type = 4, clip.right.labs = FALSE)
rpart.plot(fit_fsdf, extra = 108, type = 4, clip.right.labs = FALSE)
rpart.plot(fit_fsdf, extra = 108, type = 3, clip.right.labs = FALSE)
fit_fsdf <- rpart(good ~ mean_year_precision + precision_growth_rate + PropYrConverged,
method = 'class',
data = fsdf)
#plot(fit_fsdf, uniform=TRUE,
#     main = "Which models are good or bad (precision and convergence)?",
#     margin = .1)
#text(fit_fsdf, use.n = TRUE,
#     all = TRUE,
#     cex = .7)
rpart.plot(fit_fsdf, extra = 101, type = 4)
#plot(fit_fsdf, uniform=TRUE,
#     main = "Which models are good or bad (precision and convergence)?",
#     margin = .1)
#text(fit_fsdf, use.n = TRUE,
#     all = TRUE,
#     cex = .7)
rpart.plot(fit_fsdf, extra = 101, type = 3, clip.right.labs = FALSE)
#plot(fit_fsdf, uniform=TRUE,
#     main = "Which models are good or bad (precision and convergence)?",
#     margin = .1)
#text(fit_fsdf, use.n = TRUE,
#     all = TRUE,
#     cex = .7)
rpart.plot(fit_fsdf, extra = 104, type = 3, clip.right.labs = FALSE)
#plot(fit_fsdf, uniform=TRUE,
#     main = "Which models are good or bad (precision and convergence)?",
#     margin = .1)
#text(fit_fsdf, use.n = TRUE,
#     all = TRUE,
#     cex = .7)
rpart.plot(fit_fsdf, extra = 108, type = 3, clip.right.labs = FALSE)
#plot(fit_fsdf, uniform=TRUE,
#     main = "Which models are good or bad (precision and convergence)?",
#     margin = .1)
#text(fit_fsdf, use.n = TRUE,
#     all = TRUE,
#     cex = .7)
rpart.plot(fit_fsdf, extra = 108, type = 4, clip.right.labs = FALSE)
ggplot() +
geom_point(data=trendsData[trendsData$mean_year_precision>6&
trendsData$mean_year_precision<1000000,],
aes(x=PropYrConverged,
y=mean_year_precision),
color='#DFDFDF',
size=.5) +
geom_point(data=fsdf,
aes(x=PropYrConverged,
y=mean_year_precision,
color=as.factor(score)),
size = 2) +
scale_x_continuous(limits=c(0,1)) +
scale_y_log10(limits=c(6,1000000),labels = scales::comma) +
theme_light() +
scale_color_manual(values=c('#ca0020','#f4a582',"#92c5de", "#0571b0"),
name='Model score') +
labs(title='What determines a "bad" model?') +
xlab("Proportion of Years which Converged") + ylab("Mean Year Precision") +
geom_segment(aes(x=0.34, y = 149, xend = 0, yend = 149), color = 'red') +
geom_segment(aes(x=0.34, y = 60, xend = 1, yend = 60), color = 'red') +
geom_segment(aes(x=0.34, y = 60, xend = 0.34, yend = 149), color = 'red') +
geom_segment(aes(x=0, y = 87, xend = 1, yend = 87), color = 'red', linetype = 2)
violin <- function(y,title=NULL){
ggplot(td_taxa, aes(x=Taxa,y=y)) +
geom_violin(trim=TRUE) +
scale_y_continuous(limits = c(0,1),breaks=c(seq(0,1,.2))) +
theme(axis.text.x = element_text(angle=90, hjust=1, vjust = 0.3)) +
ggtitle(title)
}
# Subset data to remove last 10 yr data, as it just makes the plot messier
td_taxa <- trendsData[as.character(trendsData$Taxa)==
as.character(trendsData$Taxa_Root),]
td_sub <- trendsData[,c('P90','visits_P90','prop_of_years',
'prop_repeats_grp','prop_list_one',
'prop_abs','Spnvisits')]
pairs.panels(td_sub, hist.col = 'blue',smooth = TRUE)
# Read in the metrics
RM <- read.csv('Results/metrics/ALL_rawMetrics.csv')
RM <- RM[RM$Taxa==as.character(RM$Taxa_Root),]
# Read in the metrics calculated for all excluded records
RM_1rec <- read.csv('Results/metrics/ALL_1rec.csv')
colnames(RM_1rec)[2] <- 'numrec_removed'
RM <- merge(RM,RM_1rec)
RM$Taxa <- as.character(RM$Taxa)
df <- NULL
# Calculate proportion of records removed for each taxonomic group
for(taxa in sort(unique(RM$Taxa))){
num_inc <- sum(RM$Spnvisits[RM$Taxa==taxa])
num_exc <- sum(RM$numrec_removed[RM$Taxa==taxa])
df <- rbind(df,
data.frame(taxa = taxa,
Removed = (num_exc/(num_inc+num_exc)),
Included = (num_inc/(num_inc+num_exc))))
}
taxa_melt <- melt(df, id=c('taxa'))
df2 <- NULL
for(taxa in sort(unique(RM$Taxa))){
num_inc <- sum(RM$Spnvisits[RM$Taxa==taxa])
num_exc <- sum(RM$numrec_removed[RM$Taxa==taxa])
df2 <- rbind(df2,
data.frame(taxa = taxa,
RecordsIncluded = num_inc,
RecordsExcluded = num_exc,
TotalRecords = num_inc + num_exc))
}
ggplot(data = df2, aes(TotalRecords)) + geom_histogram(binwidth = 2.5e4)
ggplot(data = df2) +
geom_point(aes(x = TotalRecords, y = RecordsIncluded/TotalRecords), colour = 'blue') +
ggtitle("Relationship of  proportion of records included to number of records") +
theme_bw()
# Plot the results
stack_records <- function(df,colours,ylabel,title){
ggplot(data=df,
aes(fill=variable,y=value,x=taxa)) +
geom_bar(stat='identity',alpha = .5) +
theme(axis.text.x = element_text(angle=90, hjust=1, vjust = 0.3)) +
labs(fill = 'Data',x='Taxonomic Group',y=ylabel) +
scale_fill_manual(values = colours) +
ggtitle(title) +
scale_x_discrete(limits = rev(levels(df$taxa))) +
coord_flip()
}
stack_records(taxa_melt, colours = c('red', '#9999FF'),
ylabel = 'Proportion of records',
title = 'Proportion of records which are included')
trendsData$bad <- rep('bad',nrow(trendsData))
trendsData$bad[(trendsData$PropYrConverged < 0.3437 &
trendsData$mean_year_precision >= 149.3) |
(trendsData$PropYrConverged >= 0.3437 &
trendsData$mean_year_precision >= 60.02)] <- ' good'
# Perform the fit.  The loss parameter in the rpart function determines the
# weighting:
#   By increasing 'goodweight', this increases the importance of correctly
#   identifying good models and not assigning bad models to the good category.
#   However, this is at the expense of assigning many good models to the bad
#   category.
#   By increasing 'badweight', the inverse is true
badweight <- 1
goodweight <- 1
fit <- rpart(bad ~ median + P90 + zmedian + zP90 +
visits_median + visits_P70 + visits_P80 + visits_P90 +
prop_of_years + prop_repeats_grp + prop_list_one +
prop_abs,
method = "class",
data = trendsData,
parms = list(loss = matrix(c(0,goodweight,badweight,0),ncol = 2)))
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits
# plot tree
plot(fit, uniform=TRUE,
main = "'Rules of thumb' for species occupancy modelling",
margin = .1)
text(fit, use.n = TRUE,
all = TRUE,
cex = .7)
# prune the tree
pfit <- prune(fit, cp = fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
# plot the pruned tree
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 3, clip.right.labs = FALSE, branch=0.7)
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 4, clip.right.labs = FALSE, branch=0.7)
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = T, type = 4, clip.right.labs = FALSE, branch=0.7)
#plot(pfit, uniform=TRUE,
#  	main="Pruned 'Rules of thumb' for species occupancy modelling",
#  	margin = .1)
#text(pfit, use.n = TRUE, all = TRUE, cex = .7)
rpart.plot(pfit, extra = 108, under = F, type = 4, clip.right.labs = FALSE, branch=0.7)
RM <- read.csv('Results/metrics/ALL_rawMetrics.csv')
LM <- read.csv('Results/metrics/ALL_posteriorLM.csv')
MM <- read.csv('Results/metrics/model_data.csv')
# Remove repeat variables from model data
MM <- MM[,c('speciesName','FirstYrConverged',
'LastYrConverged','PropYrConverged')]
# These merges drop a LOT of species that don't match. That is OK as the ones that don't
# match do not meet the data minimum criteria or we do not have model data for them, so
# cannot use them for predictions.
trendsData <- merge(x = LM, y = RM,
by.x = 'species',
by.y = 'species')
trendsData <- merge(x = MM, y = trendsData,
by.x= 'speciesName', by.y = 'species')
head(trendsData)
head(trendsData)
head(LM)
head(RM)
head(MM)
dim(trendsData)
files_are_here <-
"W:/PYWELL_SHARED/Pywell Projects/BRC/Charlie/1.c. New Model Rerun/4. Outputs/CIRRUS"
first_data_file <- 'Ants/20000_update/FORMICA aquilonia.rdata'
# Load the file
load(file.path(files_are_here,first_data_file))
# Load the summary table
bugsSummary <- out$BUGSoutput$summary
## ~~~ Useful information ~~~
speciesName <- out$SPP_NAME
str(out)
str(out$BUGSoutput)
str(out$BUGSoutput$sims.list$psi.fs)
out$BUGSoutput$sims.list$psi.fs)
out$BUGSoutput$sims.list$psi.fs
out$BUGSoutput$mean)
out$BUGSoutput$mean
out$BUGSoutput$mean$psi.fs
plot(out$BUGSoutput$mean$psi.fs)
plot(out$BUGSoutput$mean$psi.fs, ylim = c(0,1))
plot(out$BUGSoutput$mean$psi.fs)
mean(out$BUGSoutput$mean$psi.fs)
length(out$BUGSoutput$mean$psi.fs)
mean(out$BUGSoutput$mean$psi.fs)
median(out$BUGSoutput$mean$psi.fs)
head(trendsData)
table(trendsData$Taxa)
trendsData$last10years <-
substring(trendsData$Taxa,1,4)
substring(trendsData$Taxa,1,4)
substring(trendsData$Taxa,1,4)=="last"
trendsData$last10years <- substring(trendsData$Taxa,1,4)=="last"
trendsData$last10yr <- substring(trendsData$Taxa,1,8) == "last10yr"
sum(trendsData$P90==0)
trendsData <- trendsData[trendsData$P90>0,]
table(trendsData$Taxa_Root,trendsData$last10yr)
trendsData <- merge(x = MM, y = trendsData,
by.x= 'speciesName', by.y = 'species')
# These merges drop a LOT of species that don't match. That is OK as the ones that don't
# match do not meet the data minimum criteria or we do not have model data for them, so
# cannot use them for predictions.
trendsData <- merge(x = LM, y = RM,
by.x = 'species',
by.y = 'species')
trendsData <- merge(x = MM, y = trendsData,
by.x= 'speciesName', by.y = 'species')
table(trendsData$Taxa_Root,trendsData$last10yr)
trendsData$last10yr <- substring(trendsData$Taxa,1,8) == "last10yr"
table(trendsData$Taxa_Root,trendsData$last10yr)
